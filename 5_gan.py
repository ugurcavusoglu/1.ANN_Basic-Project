# Import necessary libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, LeakyReLU, Conv2D, UpSampling2D, BatchNormalization
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")  # Ignore warnings

# Hyperparameters
epochs = 1000  # Number of training epochs
batch_size = 64  # Batch size for training
half_batch = batch_size // 2  # Half batch size for discriminator training
z_dim = 100  # Dimension of the noise vector (input to generator)

# Load and preprocess MNIST dataset
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
x_train = (x_train / 127.5) - 1.0  # Normalize images to [-1, 1]
x_train = np.expand_dims(x_train, axis=-1)  # Reshape images to (28, 28, 1)

# Build Discriminator model
def build_discriminator():
    inputs = Input(shape=(28, 28, 1))
    x = Conv2D(64, kernel_size=3, strides=2, padding="same")(inputs)
    x = LeakyReLU(negative_slope=0.2)(x)
    x = Conv2D(128, kernel_size=3, strides=2, padding="same")(x)
    x = LeakyReLU(negative_slope=0.2)(x)
    x = Flatten()(x)
    outputs = Dense(1, activation="sigmoid")(x)

    model = Model(inputs, outputs)
    model.compile(loss="binary_crossentropy", optimizer=Adam(0.0002, 0.5), metrics=["accuracy"])
    return model

# Build Generator model
def build_generator():
    inputs = Input(shape=(z_dim,))
    x = Dense(128 * 7 * 7, activation="relu")(inputs)
    x = Reshape((7, 7, 128))(x)
    x = UpSampling2D()(x)
    x = Conv2D(128, kernel_size=3, padding="same")(x)
    x = BatchNormalization(momentum=0.8)(x)
    x = LeakyReLU(negative_slope=0.2)(x)
    x = UpSampling2D()(x)
    x = Conv2D(64, kernel_size=3, padding="same")(x)
    x = BatchNormalization(momentum=0.8)(x)
    x = LeakyReLU(negative_slope=0.2)(x)
    x = Conv2D(1, kernel_size=3, padding="same", activation="tanh")(x)

    model = Model(inputs, x)
    return model

# Build GAN model
def build_gan(generator, discriminator):
    discriminator.trainable = False  # Freeze discriminator during GAN training
    inputs = Input(shape=(z_dim,))
    fake_images = generator(inputs)
    outputs = discriminator(fake_images)

    model = Model(inputs, outputs)
    model.compile(loss="binary_crossentropy", optimizer=Adam(0.0002, 0.5))
    return model

# Create models
discriminator = build_discriminator()
generator = build_generator()
generator.compile(loss="binary_crossentropy", optimizer=Adam(0.0002, 0.5))
gan = build_gan(generator, discriminator)
print(gan.summary())

# Check if discriminator is compiled properly
if discriminator.loss is None:
    print("❌ ERROR: Discriminator loss is None! Model is not compiled properly.")
else:
    print("✅ Discriminator successfully compiled!")

# Function to plot generated images
def plot_generated_images(generator, epoch, examples=10, dim=(1, 10), figsize=(10, 1)):
    """
    Visualize fake images generated by the generator at the end of each epoch.
    
    Args:
    - generator: Generator model of the GAN
    - epoch: Current epoch in training
    - examples: Number of fake images to generate
    - dim: Layout of images in the plot
    - figsize: Size of the visualization
    """
    # Generate noise vectors
    noise = np.random.normal(0, 1, (examples, z_dim))
    # Generate fake images using the generator
    gen_images = generator.predict(noise, verbose=0)
    
    # Normalize images to [0, 1]
    gen_images = 0.5 * gen_images + 0.5
    
    # Plot settings
    plt.figure(figsize=figsize)
    
    for i in range(gen_images.shape[0]):
        plt.subplot(dim[0], dim[1], i + 1)
        plt.imshow(gen_images[i, :, :, 0], cmap="gray")
        plt.axis("off")
    
    plt.tight_layout()
    plt.suptitle(f"Epoch: {epoch}", fontsize=16)
    plt.show()

# Loss function for training
loss_fn = tf.keras.losses.BinaryCrossentropy()

# Training loop
for epoch in tqdm(range(epochs), desc="Training Process"):
    # Train discriminator
    discriminator.trainable = True

    # Train discriminator on real images
    idx = np.random.randint(0, x_train.shape[0], half_batch)
    real_images = x_train[idx]  # Real images
    real_labels = np.ones((half_batch, 1))  # Real labels (1)

    # Generate fake images
    noise = np.random.normal(0, 1, (half_batch, z_dim))  # Noise vectors
    fake_images = generator.predict(noise, verbose=0)  # Fake images
    fake_labels = np.zeros((half_batch, 1))  # Fake labels (0)

    # Train discriminator using GradientTape
    with tf.GradientTape() as tape:
        real_preds = discriminator(real_images, training=True)
        fake_preds = discriminator(fake_images, training=True)
        
        real_loss = loss_fn(real_labels, real_preds)
        fake_loss = loss_fn(fake_labels, fake_preds)
        
        d_loss = 0.5 * (real_loss + fake_loss)

    grads = tape.gradient(d_loss, discriminator.trainable_variables)

    # Check if gradients are valid
    if grads is None or None in grads:
        print("❌ ERROR: Gradient calculation failed! Check model.trainable_variables.")
        break

    discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))

    # Train generator (GAN)
    discriminator.trainable = False  # Freeze discriminator during GAN training
    noise = np.random.normal(0, 1, (batch_size, z_dim))
    valid_labels = np.ones((batch_size, 1))  # Valid labels (1)

    with tf.GradientTape() as tape:
        g_preds = discriminator(generator(noise, training=True), training=True)
        g_loss = loss_fn(valid_labels, g_preds)

    grads = tape.gradient(g_loss, generator.trainable_variables)

    # Check if gradients are valid
    if grads is None or None in grads:
        print("❌ ERROR: Generator Gradient calculation failed! Check model.trainable_variables.")
        break

    generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))

    # Print training status
    if epoch % 100 == 0:
        print(f"{epoch}/{epochs} - D loss: {d_loss.numpy():.4f}, G loss: {g_loss.numpy():.4f}")
        # Visualize generated images
        plot_generated_images(generator, epoch)